{
 "cells": [
  {
   "cell_type": "code",
   "id": "b4b114c678f9b389",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T23:43:37.922448Z",
     "start_time": "2025-01-14T23:43:15.372298Z"
    }
   },
   "source": [
    "from typing import List, Optional, Tuple,Dict\n",
    "import torch\n",
    "import outlines\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, constr, conint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "model = outlines.models.transformers(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"token\": os.getenv(\"HF_TOKEN\")},\n",
    "    device=\"cuda\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e16f964c3fb145f6bff66c3e340aac76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:43:40.197220Z",
     "start_time": "2025-01-14T23:43:37.923455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class Severity(str, Enum):\n",
    "    minimal = \"minimal\"\n",
    "    significant = \"significant\"\n",
    "\n",
    "\n",
    "class ProblematicSnippet(BaseModel):\n",
    "    original_text: str\n",
    "    severity: Severity\n",
    "    start: Optional[conint(ge=0)] = None\n",
    "    end: Optional[conint(ge=0)] = None\n",
    "\n",
    "\n",
    "class TextAnalysis(BaseModel):\n",
    "    problematic_snippets: List[ProblematicSnippet]\n",
    "\n",
    "\n",
    "class Token(BaseModel):\n",
    "    text: str\n",
    "    start: conint(ge=0)\n",
    "    end: conint(ge=0)\n",
    "    severity: Optional[Severity] = None\n",
    "\n",
    "\n",
    "generator = outlines.generate.json(\n",
    "    model, TextAnalysis, sampler=outlines.samplers.multinomial(temperature=0.1)\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{attribute_definition}\n",
    "You are provided with a comment which contains {attribute}. Output the snippets which lead to this classification.\n",
    "----------\n",
    "Comment in question:\n",
    "{comment}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_text_analysis(attribute_definition: str, attribute: str, comment: str):\n",
    "    return generator(\n",
    "        prompt_template.format(attribute_definition=attribute_definition, attribute=attribute, comment=comment))"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:43:42.557691Z",
     "start_time": "2025-01-14T23:43:40.197220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"timonziegenbein/appropriateness-corpus\")[\"test\"]"
   ],
   "id": "5ca6ba70807bedec",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:44:42.705985Z",
     "start_time": "2025-01-14T23:43:42.557691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from constants import ATTRIBUTE_DEFINITIONS\n",
    "\n",
    "def analyse_post(post_text: str) -> Dict[str,List[Tuple[str, float]]]:\n",
    "    results = {}\n",
    "    for attribute_name, attribute_definition in ATTRIBUTE_DEFINITIONS.items():\n",
    "        results[attribute_name] = analyse_post_for_attribute(post_text, attribute_name, attribute_definition)\n",
    "    return results\n",
    "\n",
    "def analyse_post_for_attribute(post_text: str, attribute_name: str, attribute_definition: str) -> List[Tuple[str, float]]:\n",
    "    analysis_result = generate_text_analysis(attribute_name, attribute_definition, post_text)\n",
    "    update_snippet_positions(analysis_result.problematic_snippets, post_text)\n",
    "    token_list = tokenize_post_text(post_text)\n",
    "    assign_severity_to_tokens(token_list, analysis_result.problematic_snippets)\n",
    "    return convert_tokens_to_output(token_list)\n",
    "\n",
    "\n",
    "def update_snippet_positions(snippets: List[ProblematicSnippet], post_text: str) -> None:\n",
    "    for snippet in snippets:\n",
    "        re_result = re.search(snippet.original_text, post_text)\n",
    "        snippet.start = re_result.start()\n",
    "        snippet.end = re_result.end()\n",
    "\n",
    "\n",
    "def tokenize_post_text(post_text: str) -> List[Token]:\n",
    "    tokenized_text = model.tokenizer.encode(post_text)[0][0]\n",
    "    seek_start = 0\n",
    "    token_list = []\n",
    "    for token_id in tokenized_text:\n",
    "        text = model.tokenizer.decode([token_id])[0]\n",
    "        if text:\n",
    "            token = Token(text=text, start=seek_start, end=seek_start + len(text))\n",
    "            seek_start += len(text)\n",
    "            token_list.append(token)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def assign_severity_to_tokens(token_list: List[Token], snippets: List[ProblematicSnippet]) -> None:\n",
    "    for token in token_list:\n",
    "        for snippet in snippets:\n",
    "            if snippet.start <= token.start and token.end <= snippet.end:\n",
    "                token.severity = snippet.severity\n",
    "\n",
    "\n",
    "def convert_tokens_to_output(token_list: List[Token]) -> List[Tuple[str, float]]:\n",
    "    severity_mapping = {Severity.minimal: 0.5, Severity.significant: 1.0}\n",
    "    return [(token.text, severity_mapping.get(token.severity, 0.0)) for token in token_list]\n"
   ],
   "id": "bdc7c6804626ca13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('students', 0.0),\n",
       " (' should', 0.0),\n",
       " (' wear', 0.0),\n",
       " (' what', 0.5),\n",
       " (' they', 0.5),\n",
       " (' like', 0.5),\n",
       " (' and', 0.0),\n",
       " (' feel', 0.0),\n",
       " (' free', 0.5),\n",
       " (' about', 0.5),\n",
       " (' their', 0.5),\n",
       " (' clothes', 0.5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
