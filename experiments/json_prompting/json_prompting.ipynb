{
 "cells": [
  {
   "cell_type": "code",
   "id": "b4b114c678f9b389",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T23:42:52.107020Z",
     "start_time": "2025-01-15T23:42:23.812900Z"
    }
   },
   "source": [
    "from typing import List, Optional, Tuple, Dict\n",
    "import torch\n",
    "import outlines\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, constr, conint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "model = outlines.models.transformers(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"token\": os.getenv(\"HF_TOKEN\")},\n",
    "    device=\"cuda\",\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0601eb6320ae4d769a37b0371227f977"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:42:54.324824Z",
     "start_time": "2025-01-15T23:42:52.108272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class Severity(str, Enum):\n",
    "    minimal = \"minimal\"\n",
    "    significant = \"significant\"\n",
    "\n",
    "\n",
    "class ProblematicSnippet(BaseModel):\n",
    "    original_text: str\n",
    "    severity: Severity\n",
    "    start: Optional[conint(ge=0)] = None\n",
    "    end: Optional[conint(ge=0)] = None\n",
    "\n",
    "\n",
    "class TextAnalysis(BaseModel):\n",
    "    problematic_snippets: List[ProblematicSnippet]\n",
    "\n",
    "\n",
    "class Token(BaseModel):\n",
    "    text: str\n",
    "    start: conint(ge=0)\n",
    "    end: conint(ge=0)\n",
    "    severity: Optional[Severity] = None\n",
    "\n",
    "\n",
    "generator = outlines.generate.json(\n",
    "    model, TextAnalysis, sampler=outlines.samplers.multinomial(temperature=0.1)\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{attribute_definition}\n",
    "You are provided with a comment which contains {attribute}. Output the snippets which lead to this classification.\n",
    "----------\n",
    "Comment in question:\n",
    "{comment}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_text_analysis(attribute_definition: str, attribute: str, comment: str):\n",
    "    return generator(\n",
    "        prompt_template.format(attribute_definition=attribute_definition, attribute=attribute, comment=comment))"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:42:56.900463Z",
     "start_time": "2025-01-15T23:42:54.324824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"timonziegenbein/appropriateness-corpus\")[\"test\"]"
   ],
   "id": "5ca6ba70807bedec",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:42:56.909859Z",
     "start_time": "2025-01-15T23:42:56.900463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from constants import ATTRIBUTE_DEFINITIONS\n",
    "\n",
    "\n",
    "def analyse_post_for_attribute(post_text: str, attribute_name: str, attribute_definition: str) -> List[\n",
    "    Tuple[str, float]]:\n",
    "    analysis_result = generate_text_analysis(attribute_name, attribute_definition, post_text)\n",
    "    update_snippet_positions(analysis_result.problematic_snippets, post_text)\n",
    "    token_list = tokenize_post_text(post_text)\n",
    "    assign_severity_to_tokens(token_list, analysis_result.problematic_snippets)\n",
    "    return convert_tokens_to_output(token_list)\n",
    "\n",
    "\n",
    "def update_snippet_positions(snippets: List[ProblematicSnippet], post_text: str) -> None:\n",
    "    for snippet in snippets:\n",
    "        re_result = re.search(snippet.original_text, post_text)\n",
    "        if re_result is None:\n",
    "            continue\n",
    "        snippet.start = re_result.start()\n",
    "        snippet.end = re_result.end()\n",
    "    snippets[:] = [snippet for snippet in snippets if snippet.start is not None]\n",
    "\n",
    "\n",
    "def tokenize_post_text(post_text: str) -> List[Token]:\n",
    "    tokenized_text = model.tokenizer.encode(post_text)[0][0]\n",
    "    seek_start = 0\n",
    "    token_list = []\n",
    "    for token_id in tokenized_text:\n",
    "        text = model.tokenizer.decode([token_id])[0]\n",
    "        if text:\n",
    "            token = Token(text=text, start=seek_start, end=seek_start + len(text))\n",
    "            seek_start += len(text)\n",
    "            token_list.append(token)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def assign_severity_to_tokens(token_list: List[Token], snippets: List[ProblematicSnippet]) -> None:\n",
    "    for token in token_list:\n",
    "        for snippet in snippets:\n",
    "            if snippet.start <= token.start and token.end <= snippet.end:\n",
    "                token.severity = snippet.severity\n",
    "\n",
    "\n",
    "def convert_tokens_to_output(token_list: List[Token]) -> List[Tuple[str, float]]:\n",
    "    severity_mapping = {Severity.minimal: 0.5, Severity.significant: 1.0}\n",
    "    return [(token.text, severity_mapping.get(token.severity, 0.0)) for token in token_list]\n"
   ],
   "id": "bdc7c6804626ca13",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:50:02.738639Z",
     "start_time": "2025-01-15T23:42:56.910864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_file = \"testset_prediction_text.csv\"\n",
    "reader = csv.DictReader(open(csv_file))\n",
    "result = []\n",
    "# test with only first 10 rows\n",
    "counter = 0\n",
    "for line_dict in tqdm(reader):\n",
    "    post_text = line_dict[\"Text\"]\n",
    "    all_tokens = tokenize_post_text(post_text)\n",
    "    all_tokens_as_zero = [(token.text, 0.0) for token in all_tokens]\n",
    "    line_result = {}\n",
    "    for attribute_name, attribute_definition in ATTRIBUTE_DEFINITIONS.items():\n",
    "        if int(float(line_dict[attribute_name])) == 0:\n",
    "            result_tuple = (0, all_tokens_as_zero)\n",
    "            line_result[attribute_name] = str(result_tuple)\n",
    "        else:\n",
    "            print(counter, attribute_name)\n",
    "            result_tuple = (1, analyse_post_for_attribute(post_text, attribute_name, attribute_definition))\n",
    "            line_result[attribute_name] = str(result_tuple)\n",
    "    result.append(line_result)\n",
    "    counter += 1\n",
    "    if counter == 4:\n",
    "        break"
   ],
   "id": "6e9eacdd81e2c1b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Toxic Emotions\n",
      "3 Missing Commitment\n",
      "3 Missing Seriousness\n",
      "3 Missing Openness\n",
      "3 Missing Relevance\n",
      "3 Other Reasons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [07:05, 141.94s/it]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:50:02.754726Z",
     "start_time": "2025-01-15T23:50:02.739644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert to dataframe and then save as csv\n",
    "df = pd.DataFrame(result)\n",
    "df.to_csv(\"testset_prediction_text_result.csv\", index=False)"
   ],
   "id": "cbf642ebaa336bf0",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
